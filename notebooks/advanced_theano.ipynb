{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Theano in Keras\n",
    "\n",
    "To illustrate how Keras can allow us to build advanced models in Theano concisely, lets build both a fully-connected as well as a deep convolutional neural network to classify hand-written digits into their classes. \n",
    "\n",
    "Let's first start by building a fully-connected network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 28, 28)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 784)           0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 100)           78500       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 10)            1010        dense_1[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 79510\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "\n",
    "# define model architecture\n",
    "input_ = Input(shape=(28, 28))\n",
    "flat_input = Flatten()(input_)\n",
    "hidden = Dense(100, activation='relu')(flat_input)\n",
    "output = Dense(10, activation='softmax')(hidden)\n",
    "\n",
    "# build model and compile\n",
    "model = Model(input=[input_], output=[output])\n",
    "model.compile(\n",
    "    optimizer='sgd',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets load the data and do some preprocessing to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "\n",
    "def preprocess_data(x, y):\n",
    "    \n",
    "    # conver to float32, normalize, and one-hot \n",
    "    x = np.asarray(x, dtype='float32') / 256.\n",
    "    y = to_categorical(y, nb_classes=10) \n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# load the data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# preprocess the data\n",
    "x_train, y_train = preprocess_data(x_train, y_train)\n",
    "x_test, y_test = preprocess_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can pass this data to our model so that it can fit to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/10\n",
      "42000/42000 [==============================] - 1s - loss: 0.7831 - acc: 0.8034 - val_loss: 0.4183 - val_acc: 0.8881\n",
      "Epoch 2/10\n",
      "42000/42000 [==============================] - 2s - loss: 0.3793 - acc: 0.8961 - val_loss: 0.3360 - val_acc: 0.9057\n",
      "Epoch 3/10\n",
      "42000/42000 [==============================] - 2s - loss: 0.3208 - acc: 0.9103 - val_loss: 0.3011 - val_acc: 0.9152\n",
      "Epoch 4/10\n",
      "42000/42000 [==============================] - 1s - loss: 0.2887 - acc: 0.9193 - val_loss: 0.2790 - val_acc: 0.9207\n",
      "Epoch 5/10\n",
      "42000/42000 [==============================] - 1s - loss: 0.2653 - acc: 0.9257 - val_loss: 0.2599 - val_acc: 0.9264\n",
      "Epoch 6/10\n",
      "42000/42000 [==============================] - 1s - loss: 0.2465 - acc: 0.9315 - val_loss: 0.2457 - val_acc: 0.9311\n",
      "Epoch 7/10\n",
      "42000/42000 [==============================] - 2s - loss: 0.2312 - acc: 0.9345 - val_loss: 0.2346 - val_acc: 0.9341\n",
      "Epoch 8/10\n",
      "42000/42000 [==============================] - 2s - loss: 0.2178 - acc: 0.9389 - val_loss: 0.2248 - val_acc: 0.9363\n",
      "Epoch 9/10\n",
      "42000/42000 [==============================] - 2s - loss: 0.2059 - acc: 0.9416 - val_loss: 0.2125 - val_acc: 0.9411\n",
      "Epoch 10/10\n",
      "42000/42000 [==============================] - 2s - loss: 0.1956 - acc: 0.9448 - val_loss: 0.2046 - val_acc: 0.9426\n"
     ]
    }
   ],
   "source": [
    "# fit model to the data\n",
    "hist = model.fit(\n",
    "    x=x_train, y=y_train, batch_size=32,\n",
    "    nb_epoch=10, verbose=1, validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save model history after fitting          \n",
    "with open('model_history.txt', 'w') as save_file:\n",
    "    json.dump(hist.history, save_file)\n",
    "save_file.close()\n",
    "\n",
    "# save model architecture, weights, and config\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that with a very simple model, we get very reasonable results on MNIST classificaiton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a comparison and to introduce some more complex layers, we can also build a deep convolutional neural network with relative ease. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 1, 28, 28)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_1 (Convolution2D)  (None, 32, 24, 24)    832         input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 32, 24, 24)    64          convolution2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32, 24, 24)    0           batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_1 (MaxPooling2D)    (None, 32, 12, 12)    0           activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_2 (Convolution2D)  (None, 64, 10, 10)    18496       maxpooling2d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 64, 10, 10)    128         convolution2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 64, 10, 10)    0           batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_2 (MaxPooling2D)    (None, 64, 5, 5)      0           activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_3 (Convolution2D)  (None, 10, 5, 5)      650         maxpooling2d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 10, 5, 5)      10          convolution2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 10, 5, 5)      0           batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "globalaveragepooling2d_1 (Global (None, 10)            0           activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 10)            0           globalaveragepooling2d_1[0][0]   \n",
      "====================================================================================================\n",
      "Total params: 20180\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization, Activation, GlobalAveragePooling2D\n",
    "\n",
    "# define model architecture\n",
    "input_ = Input(shape=(1, 28, 28))\n",
    "\n",
    "net = Convolution2D(32, 5, 5, activation='linear')(input_)\n",
    "net = BatchNormalization(mode=0, axis=1)(net)\n",
    "net = Activation('relu')(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "\n",
    "net = Convolution2D(64, 3, 3, activation='linear')(net)\n",
    "net = BatchNormalization(mode=0, axis=1)(net)\n",
    "net = Activation('relu')(net)\n",
    "net = MaxPooling2D(pool_size=(2, 2))(net)\n",
    "\n",
    "net = Convolution2D(10, 1, 1, activation='linear')(net)\n",
    "net = BatchNormalization(mode=0, axis=-1)(net)\n",
    "net = Activation('relu')(net)\n",
    "\n",
    "net = GlobalAveragePooling2D()(net)\n",
    "output = Activation('softmax')(net)\n",
    "\n",
    "\n",
    "# build model and compile\n",
    "model = Model(input=[input_], output=[output])\n",
    "model.compile(\n",
    "    optimizer='nadam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets fit this model to the data and compare it's performance to the fully-connected model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42000 samples, validate on 18000 samples\n",
      "Epoch 1/5\n",
      "42000/42000 [==============================] - 123s - loss: 0.7991 - acc: 0.8632 - val_loss: 0.3653 - val_acc: 0.9277\n",
      "Epoch 2/5\n",
      "42000/42000 [==============================] - 114s - loss: 0.2477 - acc: 0.9586 - val_loss: 0.2549 - val_acc: 0.9387\n",
      "Epoch 3/5\n",
      "42000/42000 [==============================] - 114s - loss: 0.1513 - acc: 0.9704 - val_loss: 0.1647 - val_acc: 0.9536\n",
      "Epoch 4/5\n",
      "42000/42000 [==============================] - 114s - loss: 0.1132 - acc: 0.9742 - val_loss: 0.5960 - val_acc: 0.7899\n",
      "Epoch 5/5\n",
      "42000/42000 [==============================] - 113s - loss: 0.0886 - acc: 0.9792 - val_loss: 0.1062 - val_acc: 0.9699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1122cd8d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# expand channel dimension of data\n",
    "x_train = np.expand_dims(x_train, axis=1)\n",
    "x_test = np.expand_dims(x_test, axis=1)\n",
    "\n",
    "# fit model to the data\n",
    "model.fit(\n",
    "    x=x_train, y=y_train, batch_size=32,\n",
    "    nb_epoch=5, verbose=1, validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
